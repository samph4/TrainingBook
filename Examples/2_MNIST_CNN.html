

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2 - MNIST CNN Network &#8212; Ai Training Manual for Maisie &amp; Filipe</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Introduction to Python" href="../Python%20Help/Introduction%20to%20Python.html" />
    <link rel="prev" title="1 - MNIST MLP Network" href="1_MNIST_MLP.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/EdLogo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Ai Training Manual for Maisie & Filipe</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  JupyterBook
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown.html">
   Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks.html">
   Content with notebooks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning Examples
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1_MNIST_MLP.html">
   1 - MNIST MLP Network
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2 - MNIST CNN Network
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Python Support
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Python%20Help/Introduction%20to%20Python.html">
   Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python%20Help/Useful%20Python%20Commands.html">
   Useful Packages / Commands
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Examples/2_MNIST_CNN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/Examples/2_MNIST_CNN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/executablebooks/jupyter-book/blob/master/Examples/2_MNIST_CNN.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preface">
   Preface
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#coding-a-mlp-network-in-keras">
   Coding a MLP network in Keras
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-necessary-libraries">
     Import Necessary Libraries
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading-your-data">
     Loading your data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interact-visualise-the-training-data">
     Interact/Visualise the training Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sorting-the-x-feature-data">
     Sorting the X (feature) data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sorting-the-y-target-data">
     Sorting the Y (target) data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-mlp">
     Creating the MLP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-the-model">
     Training the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#results-and-evaluation">
     Results and Evaluation!
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="mnist-cnn-network">
<h1>2 - MNIST CNN Network<a class="headerlink" href="#mnist-cnn-network" title="Permalink to this headline">¬∂</a></h1>
<p>Adapted from this tutorial -&gt;
https://www.machinecurve.com/index.php/2019/07/27/how-to-create-a-basic-mlp-classifier-with-the-keras-sequential-api/</p>
<p><code class="docutils literal notranslate"><span class="pre">github.com/samph4</span></code></p>
<p>~</p>
<div class="section" id="preface">
<h2>Preface<a class="headerlink" href="#preface" title="Permalink to this headline">¬∂</a></h2>
<p>By the end of this notebook you (hopefully) should have been able to make a neural network that you get to play around with. It will use a popular dataset known as the MNIST handwritten digits http://yann.lecun.com/exdb/mnist/ which is essentially a massive database containing 60,000 images of handwritten numeric digits from 0-9. We are going to create and train a classification network that will allow you to input a handwritten digit and the trained model should be able to predict or ‚Äòclassify‚Äô what digit it is that you input to the network.</p>
<p>It should be noted at this point that there are many different kinds of neural networks that have been designed to deal with a whole range of applications amidst different problem spaces from natural language processing to self driving cars etc etc. But we‚Äôll start with what is often considered the most simple of neural networks known as the Multi-layer percepron network - a diagram of which is shown below!</p>
<p><img alt="Image" src="../_images/mlp.png" /></p>
<p>These networks are excellent at making a prediction (or number of predictions), based on a number of inputs. Simply put, the MLP is a layered network where each layer consists of a number of nodes. These nodes are often referred to as Perceptrons (hence multi-layer perceptron) and they can be thought of as a place where computation occurs (see left image). These nodes take a series of inputs and convert them into a single output. Each node has two important parameters to consider known as a ‚Äòweight‚Äô and a ‚Äòbias‚Äô. A weight is something that is specific to each input that enters the node: a numeric input enters the node and is multiplied by its associated numeric weight. The sum of all of the inputs multiplied by their associated weights is summed at the node. At this point, a ‚Äòbias‚Äô is another quantity that is added to the node to modify its output. That output then passes through an activation function (we‚Äôll get to this later) and then depending on how the network is arranged that output will enter a different node in a different layer as an input! During training, the values of these weights and biases are optimised such that their outputs begin to match more closely with the task you are trying to perform. During training, if the output from your network is drastically different to the value you want it to be (the target value), then you can expect the weights and biases to be optimised more rigorously than they would be if there was no difference between its output and its target value - but again I‚Äôll talk more about this later.</p>
<p>On the right hand side of the figure, you can see that a MLP network is typically split into three key sections:</p>
<ul class="simple">
<li><p>The input layer</p></li>
<li><p>The hidden layer</p></li>
<li><p>The output layer</p></li>
</ul>
<p>The input layer, as you‚Äôd expect, is where the data is input to the network. Let‚Äôs say for example we wanted to develop a network to predict somebodies mass based on their height, daily calorific intake and gender. Then a 3-element vector input to the network might look something like <strong>[180, 2500, 0]</strong> this (where 0 = male, 1 = female). The hidden layer is everything in between and os often reffered to as the ‚Äòblack box‚Äô as it is less clear as to what the output from the nodes within this layer mean. They are the intermediary layers that the MLP uses on the way to predicting the actual output. Finally the output layer is simply the output, and the number of nodes typically corresponds to the number of outputs (or predictions) the MLP model is making. So for our example given an input of [180, 2500, 0]; we might expect a trained model to return an output (prediction) of <strong>[80]</strong> to represent 80kg.</p>
<p>~</p>
</div>
<div class="section" id="coding-a-mlp-network-in-keras">
<h2>Coding a MLP network in Keras<a class="headerlink" href="#coding-a-mlp-network-in-keras" title="Permalink to this headline">¬∂</a></h2>
<p>We use the MNIST database, which stands for Modified National Institute of Standards and Technology (LeCun et al., 1998). It is one of the standard datasets that is used throughout the machine learning community, often for educational purposes.</p>
<p>In simple English, it‚Äôs just a database of handwritten numbers that are 28 by 28 pixels. They‚Äôve been used in the early days of neural networks in one of the first practical applications of AI, being a digit recognizer for handwritten numbers. More information on MNIST is available here.</p>
<p>And this is what these numbers look like:</p>
<p><img alt="Image" src="Examples\Figures%5Cmnist1.png" /></p>
<div class="section" id="import-necessary-libraries">
<h3>Import Necessary Libraries<a class="headerlink" href="#import-necessary-libraries" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">keras</span>                            <span class="c1">#keras library that gives us access to a bunch of machine learning functions</span>
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>        <span class="c1">#keras module has a bunch of datasets that we can import directly</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>     <span class="c1">#sequential model lets us develop networks in sequence (in layers)</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>          <span class="c1">#Dense is the name Keras gives to a fully connected layer of nodes</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>  <span class="c1">#will make sense later!</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>         <span class="c1">#matplotlib is a library that lets us plot graphs like MATLAB would.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using TensorFlow backend.
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loading-your-data">
<h3>Loading your data<a class="headerlink" href="#loading-your-data" title="Permalink to this headline">¬∂</a></h3>
<p>Here we can assign some configuration variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configuration Options</span>
<span class="n">feature_vector_length</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">60000</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>                 <span class="c1"># digit can either be one of 10 values, from 0, 1, 2, 3, ..., 8, 9.</span>
</pre></div>
</div>
</div>
</div>
<p>So this is a little weird. MLPs are good at mapping a bunch of inputs to a bunch of outputs. They wouldn‚Äôt be your first choice when working with image data but I found this example and thought it was more interesting than some of the others. Typically you would use something called a Convolutional Neural Network (CNN) which we‚Äôll get too next, so we need to do something a little funky and flatten the pixel data by converting it from 28 x 28 pixel to a single vector. Hence why feature_vector_length was labeled as 784 since 28 * 28 = 784. Explained more here:</p>
<blockquote>
<div><p>‚ÄúOne MNIST sample is an image of 28 by 28 pixels. An interesting observation that I made a while ago is that MLPs don‚Äôt support multidimensional data like images natively. What you‚Äôll have to do is to flatten the image, in the sense that you‚Äôll just take all the rows and put them into a massive row. Since 28 times 28 is 784, our feature vector (which with the Pima dataset SLP was only 8) will contain 784 features (pixels).‚Äù -  https://www.machinecurve.com/index.php/2019/07/27/how-to-create-a-basic-mlp-classifier-with-the-keras-sequential-api/</p>
</div></blockquote>
<p>The MNIST dataset has 60,000 images in its training set hence the number of samples is set at 60,000. We can confirm this below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train shape is &#39;</span><span class="p">,</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X_train shape is  (60000, 28, 28)
</pre></div>
</div>
</div>
</div>
<p>Here we used the mnist.load_data function and the function is written (thanks Keras) such that we can extract the training data and test data directly. <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code> are simply conventions that are typically used to refer to the input and output data. Obviously you can call them whatever you like, but <code class="docutils literal notranslate"><span class="pre">X</span></code> tends to be used to refer to the input data and <code class="docutils literal notranslate"><span class="pre">Y</span></code> refers to the output data (or target data). So to reiterate, <code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">Y_train</span></code> contain the data used to train the network where for a given sample within X_train, the correct output (prediction) is found in the corresponding sample in <code class="docutils literal notranslate"><span class="pre">Y_train</span></code>. Similarly, the same goes for <code class="docutils literal notranslate"><span class="pre">X_test</span></code>, and <code class="docutils literal notranslate"><span class="pre">Y_test</span></code>. However this data is not used during training and is used to test the model once it has been trained on data that it has not been trained on / seen before; hence test data!</p>
<p>You can also see that I wrote <code class="docutils literal notranslate"><span class="pre">X_train.shape</span></code>, the .shape part is a built in function in Python to show the shape of the variable. We can see that the variable has 3 dimensions (60000, 28, 28) that refers to 60,000 samples in the dataset where each sample is a 28x28 matrix containing pixel data.</p>
</div>
<div class="section" id="interact-visualise-the-training-data">
<h3>Interact/Visualise the training Data<a class="headerlink" href="#interact-visualise-the-training-data" title="Permalink to this headline">¬∂</a></h3>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> module to display some of the images from the MNIST dataset. Play around with the code below. The [0] is simply indexing the first sample out of 60,000 from within the dataset. So replacing that with any number between [0, 59999] should be fine. Then the bit of code below I wrote just to show you how can you plot more images at the same time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize one sample</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Visualize range of samples</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_MNIST_CNN_13_0.png" src="../_images/2_MNIST_CNN_13_0.png" />
<img alt="../_images/2_MNIST_CNN_13_1.png" src="../_images/2_MNIST_CNN_13_1.png" />
</div>
</div>
</div>
<div class="section" id="sorting-the-x-feature-data">
<h3>Sorting the X (feature) data<a class="headerlink" href="#sorting-the-x-feature-data" title="Permalink to this headline">¬∂</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reshape the data - MLPs do not understand such things as &#39;2D&#39;.</span>
<span class="c1"># Reshape to 28 x 28 pixels = 784 features</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">feature_vector_length</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">feature_vector_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When we used the <code class="docutils literal notranslate"><span class="pre">mnist.load_data()</span></code> function, the X elements represent the feature vectors (which at that point in time are still 28√ó28 pixels); the Y elements represent the targets (at that point still being numbers, i.e. 0-9). We can therefore use the <code class="docutils literal notranslate"><span class="pre">.reshape</span></code> function to change the shape of the vector. We reshape it such that the 28x28 image is flattened into a single vector of size 784. Remember that in python the first index in a variable is 0, whereas in MATLAB it is 1. Remember that <code class="docutils literal notranslate"><span class="pre">X_train.shape</span></code> returned (60000, 28,28), therefore <code class="docutils literal notranslate"><span class="pre">X_train[0]</span></code> = 60,000, <code class="docutils literal notranslate"><span class="pre">X_train[1]</span></code>= 28 and finally <code class="docutils literal notranslate"><span class="pre">X_train[2]</span></code> also equals 28. We can therefore change the shape of <code class="docutils literal notranslate"><span class="pre">X_train</span></code> (reshape it) by calling <code class="docutils literal notranslate"><span class="pre">X_train.reshape(60,000,</span> <span class="pre">784)</span></code> which is identical to <code class="docutils literal notranslate"><span class="pre">X_train.reshape(X_train.shape[0],</span> <span class="pre">feature_vector_length)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># np.max(a) prints the maximum value in &#39;a&#39;.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Maximum value in X_train is&#39;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

<span class="c1"># Convert into greyscale</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">/=</span> <span class="mi">255</span>
<span class="n">X_test</span> <span class="o">/=</span> <span class="mi">255</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">6</span><span class="o">-</span><span class="n">f02e31240434</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># np.max(a) prints the maximum value in &#39;a&#39;.</span>
<span class="nn">----&gt; 2 print(f&#39;Maximum value</span> in <span class="ni">X_train is&#39;,np.max</span><span class="nt">(X_train))</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> 
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="c1"># Convert into greyscale</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;np&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">.astype</span></code> is a function used to change the type of variable. Here we assure that the training data are <code class="docutils literal notranslate"><span class="pre">float.32</span></code> variables which refers to a decimal with a specific degree of precision.</p>
<p>The next part simply converts the data into grayscale. The RGB colour space is often referred to as RGB 256. The red, green and blue use 8 bits each, which have integer values from 0 to 255. 0 typically corresponds to black and 255 to white. We can therefore divide all of the training data by 255 to scale the output between 0 and 1.</p>
</div>
<div class="section" id="sorting-the-y-target-data">
<h3>Sorting the Y (target) data<a class="headerlink" href="#sorting-the-y-target-data" title="Permalink to this headline">¬∂</a></h3>
<p>Lets take a quick look at what the <code class="docutils literal notranslate"><span class="pre">Y_train</span></code> data looks like. By calling <code class="docutils literal notranslate"><span class="pre">Y_train[0:10]</span></code> we can print some of the initial elements.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)
</pre></div>
</div>
</div>
</div>
<p>We see that the data corresponds directly to the value of the handwritten digit in scalar form from 0-9. I‚Äôll take a slight detour here because when we‚Äôre training the model the loss function that we are going to use is called <code class="docutils literal notranslate"><span class="pre">categorical</span> <span class="pre">cross</span> <span class="pre">entropy</span></code>. This is another example the guy uses to explain the idea a little better:</p>
<blockquote>
<div><p>‚ÄúFor those problems, we need a loss function that is called categorical crossentropy. In plain English, I always compare it with a purple elephant üêò. Suppose that the relationships in the real world (which are captured by your training date) together compose a purple elephant (a.k.a. distribution). We next train a machine learning model that attempts to be as accurate as the original data; hence attempting to classify data as that purple elephant. How well the model is capable of doing that is what is called a loss, and the loss function allows one to compare one distribution (elephant) with the other (hopefully the same elephant). Cross entropy allows one to compare those. We can‚Äôt use the binary variant (it only compares two elephants), but need the categorical one (which can compare multiple elephants). This however requires us to ‚Äòlock‚Äô the set of elephants first, to avoid that another one is added somehow. This is called categorical data: it belongs to a fixed set of categories‚Äù ~(Chollet, 2017).</p>
</div></blockquote>
<p>However, the MNIST targets here are just numbers and are not categorical. Here we can use the <code class="docutils literal notranslate"><span class="pre">to_categorical</span></code> function that we imported earlier to turn the numbers into categorical data. Therefore, rather than the target values being scalars [0, 9], one target vector will subseqently look as follows:</p>
<p><img alt="Image" src="../_images/categorical2.png" /></p>
<p>This refers to a target value of 5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert target classes to categorical ones</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="n">Y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="creating-the-mlp">
<h3>Creating the MLP<a class="headerlink" href="#creating-the-mlp" title="Permalink to this headline">¬∂</a></h3>
<p>Here we will actually start to define the architecture of the MLP network. But this part shouldn‚Äôt be scary, the most important part is making sure that we have a clear idea as to what is going into the network (input) and what is going out of it (the output). We have already spent a lot of time manipulating the data so we should already have a good understanding of the basic shape of the MLP network. We want it to take an image as input and give us the classification as output. We know that each image has been flattened into a feature vector of size 784 (28x28 pixels) so therefore the input to the network will have 784 nodes; and we know that the output will be the categorical vector described earlier containing 10 classes [0-9].</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set the input shape</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">feature_vector_length</span><span class="p">,)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature shape: </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Feature shape: (784,)
</pre></div>
</div>
</div>
</div>
<p>Here we explicitly define the length of the input to the MLP network. This looks a little weird because we already defined the feature_vector_length to be 784 (28x28) so couldn‚Äôt we just use the same thing? The answer is yes you could, but this is slightly better practice. Here we use the notation <code class="docutils literal notranslate"><span class="pre">(feature_vector_length,)</span></code>, and take note of the comma. This changes the variable type from numeric to something called a <strong>tuple</strong>.</p>
<blockquote>
<div><p>A tuple is a collection of objects which ordered and immutable. Tuples are sequences, just like lists. The differences between tuples and lists are, the tuples cannot be changed unlike lists and tuples use parentheses, whereas lists use square brackets. - https://www.tutorialspoint.com/python/python_tuples.htm</p>
</div></blockquote>
<p>Basically they contain data that cannot be changed, so here we are just saying that the input to the MLP network is of size 784 and will not change. In the cell block below, we can create the MLP network!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">350</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span> <span class="c1"># Layer 1 (input and first layer)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>                           <span class="c1"># Layer 2</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>               <span class="c1"># Layer 3 (output)</span>
</pre></div>
</div>
</div>
</div>
<p>So there‚Äôs a few things happening here but we‚Äôll go through each line and it will make more sense. So first we define the model type by writing <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Sequential()</span></code>.</p>
<blockquote>
<div><p>‚ÄúThe sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. - https://keras.io/guides/sequential_model/</p>
</div></blockquote>
<p>So since we are creating a MLP network and there is a clear input and output tensor, a sequential model is suitable. By calling <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Sequential()</span></code>, we create a variable named <code class="docutils literal notranslate"><span class="pre">model</span></code> which will contain all of the information regarding the model that we create.</p>
<p>At this point, we can add additional layers directly by calling <code class="docutils literal notranslate"><span class="pre">.add</span></code> on the model variable directly. Keras gives access to a whole bunch of different layers that you can use and play around with to do different things (https://keras.io/api/layers/), but here we use the Dense layer which means that all of the nodes in that layer will be fully connected (densely connected) to the nodes in the next layer. When we create a dense layer, we need to define a few properties - how many nodes we want in the layer, if it is the first layer we have added we need to define the input shape, and finally the activation function used to modulate the output before it passes into the next layer. So if we consider the first layer <code class="docutils literal notranslate"><span class="pre">model.add(Dense(350,</span> <span class="pre">input_shape=input_shape,</span> <span class="pre">activation='relu'))</span></code>, we can see that a dense layer has been added and it has 350 nodes. Since it is the first layer we define <code class="docutils literal notranslate"><span class="pre">input_shape=input_shape</span></code> (we named the variable <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> previously and it matches, but it could have been named anything) and finally we define a ‚Äòrelu‚Äô activation function (will get to shortly). The second layer is defined in a similar fashion but this time has 50 nodes instead and also uses the ‚Äòrelu‚Äô activation function. Then finally we define the output layer with <code class="docutils literal notranslate"><span class="pre">model.add(Dense(num_classes,</span> <span class="pre">activation='softmax'))</span></code>, where <code class="docutils literal notranslate"><span class="pre">num_classes</span></code> here defines the size of the output layer.</p>
<p><em>Activation Functions</em> - regarding activation functions, it‚Äôs best to redirect you to this link because it does a good job of explaining what they are and their different types (https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0). The important thing to know is that they are used to modulate the output of a node before it passes further into the network. So for example, the <code class="docutils literal notranslate"><span class="pre">relu</span></code> function refers to a REctified Linear Unit (ReLU) that simply allows positive values to pass through unchanged and replaces any negative values with 0. Other functions include Sigmoid functions which moderate the output between [-1, 1]. Finally, the softmax activation function is useful because it will make the model return probabilities of the networks predictions for each class in the output. The probabilities of which will sum to a total of 1.</p>
</div>
<div class="section" id="training-the-model">
<h3>Training the model<a class="headerlink" href="#training-the-model" title="Permalink to this headline">¬∂</a></h3>
<p>Finally we can train the model. Once the model architecture has been defined (by determining the layers/arrangement etc), there is one final step. We need to compile the model as this lets us define the loss function, the optimizer and the metrics used to track the results of the training process. The <code class="docutils literal notranslate"><span class="pre">model.compile()</span></code> function is powerful and allows us to change and define lots of things about the training process; here we can select whichever loss function we like, we can use any optimiser and also use any metric we wish to judge the performance of the model (see https://keras.io/api/metrics/).</p>
<p>First, let‚Äôs discuss the <ins>loss function</ins>:</p>
<blockquote>
<div><p>‚ÄúThe purpose of loss functions is to compute the quantity that a model should seek to minimize during training.‚Äù - https://keras.io/api/losses/</p>
</div></blockquote>
<p>The loss function is the function that the neural network is trying to minimise. The problem and data types you are working with will determine the loss function that you will use and that will make sense, there are many to choose from including probabilistic losses and regression losses etc. But here we use the <code class="docutils literal notranslate"><span class="pre">categorical_crossentropy</span></code> function as it computes the crossentropy loss between the labels and the predictions. We use this loss function when there are two or more label classes (we have 10). The function expects labels to be provided in a <em>one_hot</em> representation which is the format we arranged the Y (feature) data into using the <code class="docutils literal notranslate"><span class="pre">to_categorical</span></code> function - this means that [0,0,0,0,1,0,0,0,0,0] refers to the class 4 rather than the scalar value of 4 itself. This loss function performs well and that is why we took the extra step to assure the data was in this format. If we insisted that we want to provide labels as integers, we could have utilised the <code class="docutils literal notranslate"><span class="pre">SparseCategoricalCrossentropy</span></code> loss instead.</p>
<p>Next, we‚Äôll talk about the <ins>optimizer</ins>:</p>
<blockquote>
<div><p>‚ÄúOptimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use.‚Äù - https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6.</p>
</div></blockquote>
<p>Again, there are many types of optimizers available and they are essentially optimization algorithms that we use for training neural networks. This is the part where it can get very mathsy and I would recommend having a brief read through the link above as it does a good job explaining how some of the different optimizers work. But ultimately the optimizer is responsible for updating the weights and biases of the neural network in an attempt to minimise the value computed by the loss function. Here we use the <code class="docutils literal notranslate"><span class="pre">adam</span></code> optimiser which has become very popular in machine learning applications and often yields favourable results. It is a good starting point - adam stands for Adaptive Moment Estimation.</p>
<p>Finally, we have <ins>metrics</ins>:</p>
<blockquote>
<div><p>‚ÄúA metric is a function that is used to judge the performance of your model.‚Äù - https://keras.io/api/metrics/</p>
</div></blockquote>
<p>Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you can use any loss function as a metric. Metrics are useful because they are stored within the model variable itself during training and we can access them once training has finished (or during training) to plot the results to get a better understanding as to what is happening durin gthe training process. We will do this later!</p>
<p>~</p>
<p>FInally we can call the <code class="docutils literal notranslate"><span class="pre">model.fit()</span></code> function to actually begin the training process and start optimising the weights and biases to suit the problem. Here we pass in the training data, <code class="docutils literal notranslate"><span class="pre">X_train</span></code> (feature data) and <code class="docutils literal notranslate"><span class="pre">Y_train</span></code> (target data) directly into the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> function. Next we define the number of epochs, this is essentially the stopping criteria and lets the model know that it should stop training once 20 epochs have passed.</p>
<blockquote>
<div><p><em>What is an epoch?</em> ~ ‚ÄúAn epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. Datasets are usually grouped into batches (especially when the amount of data is very large). Some people use the term iteration loosely and refer to putting one batch through the model as an iteration.‚Äù - https://radiopaedia.org/articles/epoch-machine-learning</p>
</div></blockquote>
<p>Recall that the dataset consists of 60,000 samples - here we define <code class="docutils literal notranslate"><span class="pre">batch_size=250</span></code>, this means that for each iteration 250 samples are passed into the model. Larger batch sizes means that the model will take less time to train, but is dependent on the hardware you have available. Although not always necessary (depending on size of the problem) it is preferred to use GPUs to train neural networks, as the parellilsation capabilities of the graphics card architecture allow them to compute through batch data much more efficiently than CPUs (but that‚Äôs another topic in itself). If ever your machine has issues running a model, a good first step might be to try lowering the batch_size. <code class="docutils literal notranslate"><span class="pre">verbose=1</span></code> simply gives us a pretty animated progress bar for when we‚Äôre training the model, conversely setting <code class="docutils literal notranslate"><span class="pre">verbose=0</span></code> will output nothing (this happens by default). Finally, we can split some of the training dataset into a validation set used to evaluate the network further after training. The <code class="docutils literal notranslate"><span class="pre">validation_split=0.2</span></code> statement within the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> function allocates 20% of the training set for validation purposes. Running the cell below will begin training!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Configure the model and start training</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 48000 samples, validate on 12000 samples
Epoch 1/20
48000/48000 [==============================] - 1s 19us/step - loss: 0.3953 - accuracy: 0.8871 - val_loss: 0.1815 - val_accuracy: 0.9458
Epoch 2/20
48000/48000 [==============================] - 1s 12us/step - loss: 0.1458 - accuracy: 0.9578 - val_loss: 0.1292 - val_accuracy: 0.9628
Epoch 3/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.1001 - accuracy: 0.9714 - val_loss: 0.1055 - val_accuracy: 0.9673
Epoch 4/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0734 - accuracy: 0.9785 - val_loss: 0.0974 - val_accuracy: 0.9703
Epoch 5/20
48000/48000 [==============================] - 1s 12us/step - loss: 0.0551 - accuracy: 0.9848 - val_loss: 0.0921 - val_accuracy: 0.9724
Epoch 6/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0432 - accuracy: 0.9872 - val_loss: 0.0862 - val_accuracy: 0.9758
Epoch 7/20
48000/48000 [==============================] - 1s 12us/step - loss: 0.0340 - accuracy: 0.9902 - val_loss: 0.0899 - val_accuracy: 0.9740
Epoch 8/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0251 - accuracy: 0.9931 - val_loss: 0.0894 - val_accuracy: 0.9756
Epoch 9/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.0881 - val_accuracy: 0.9762
Epoch 10/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0150 - accuracy: 0.9963 - val_loss: 0.0881 - val_accuracy: 0.9787
Epoch 11/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0127 - accuracy: 0.9972 - val_loss: 0.0889 - val_accuracy: 0.9773
Epoch 12/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.0950 - val_accuracy: 0.9767
Epoch 13/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0076 - accuracy: 0.9987 - val_loss: 0.0881 - val_accuracy: 0.9782
Epoch 14/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.0939 - val_accuracy: 0.9774
Epoch 15/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0061 - accuracy: 0.9986 - val_loss: 0.0967 - val_accuracy: 0.9780
Epoch 16/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.1056 - val_accuracy: 0.9760
Epoch 17/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.0957 - val_accuracy: 0.9792
Epoch 18/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0964 - val_accuracy: 0.9793
Epoch 19/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.1474 - val_accuracy: 0.9721
Epoch 20/20
48000/48000 [==============================] - 1s 11us/step - loss: 0.0160 - accuracy: 0.9941 - val_loss: 0.0990 - val_accuracy: 0.9760
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;keras.callbacks.callbacks.History at 0x2d9e60fbd88&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="results-and-evaluation">
<h3>Results and Evaluation!<a class="headerlink" href="#results-and-evaluation" title="Permalink to this headline">¬∂</a></h3>
<p>Now the model has been trained and we can evaluate the results and play around with the trained model. I mentioned earlier that the metrics during trained are saved within the model variable itself, accessing them is not the most intuitive honestly, but by calling <code class="docutils literal notranslate"><span class="pre">dir(model)</span></code> you will see a big list of all the attributes you can call from that variable. One of them is history, within that we can call a secondary history attribute that stores all of the metrics stored during training. We can then plot these values using the <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> library which stores works in a very similar way to MATLAB.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Traiing Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">),</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Accuracy&#39;</span><span class="p">)</span>

<span class="c1"># some simple formatting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch Number&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_MNIST_CNN_30_0.png" src="../_images/2_MNIST_CNN_30_0.png" />
</div>
</div>
<p>As you can see, training loss decreases rapidly (noted by the steep gradient). This is perfectly normal, as the model always learns most during the early stages of optimization. Accuracies converge after only one epoch, and still improve during the 10th, albeit slightly.</p>
<p>Validation loss is also still decreasing during the 10th epoch. This means that although the model already performs well (accuracies over 97%!), it can still improve further without losing its power to generalize to data it has never seen. In other words, our model is still underfit‚Ä¶ perhaps, increasing the number of epochs until validation loss increases again might yield us an even better model.</p>
<p>However, this was all observed from validation data. What‚Äôs best is to test it with the actual testing data that was generated earlier. Here we call the <code class="docutils literal notranslate"><span class="pre">model.evaluate</span></code> function and input the test data we created earlier, <code class="docutils literal notranslate"><span class="pre">X_test</span></code> and <code class="docutils literal notranslate"><span class="pre">Y_test</span></code>. Once again we can use the verbose feature to display the progress output and print the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test the model after training</span>
<span class="n">test_results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test results - Loss: </span><span class="si">{</span><span class="n">test_results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1"> - Accuracy: </span><span class="si">{</span><span class="n">test_results</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10000/10000 [==============================] - 0s 41us/step
Test results - Loss: 0.08917443580453692 - Accuracy: 97.75000214576721%
</pre></div>
</div>
</div>
</div>
<p>The results show that the accuracy of the model was 97.64 % when evaluated on the test data that it had not been trained on! Obviously this is a good result, but people have managed to achieve more accurate results for this dataset and there are programming competitions and leaderboards on www.kaggle.com that offer cash prizes and incentives for those who can improve on the ‚Äòstate of the art‚Äô models. We could modify size of the network, train the model for longer, perhaps add more layers, or use different optimisers, activation functions and loss functions in attempt to improve the accuracy of the network. A better method in this case though might be to use a different type of network alltogether; a popular choice might be a Convolutional Neural Network - but we‚Äôll get to that in the next notebook.</p>
<p>For now, lets play around with the model that we just trained and take a look at how we can use it to make new predictions! First of all, we can call the <code class="docutils literal notranslate"><span class="pre">model.predict()</span></code> function to use the model (as you might expect) to make a new prediction. We can simply pass a handwritten digit image in the same way as we did during training. But first, lets actually see what we are putting into the network so we know whether it gives us the right answer!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># reshaping the test data  into image format so that we can plot it - note the 28x28 pixels.</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">10</span>                            <span class="c1"># which sample (index) should we take from the test dataset to evalutate!</span>
<span class="n">test_input</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">index</span><span class="p">,:,:]</span>        <span class="c1"># [28x28] array containing image data for a single digit taken from the dataset.</span>

<span class="c1"># Visualize the sample we will put into the network!</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_MNIST_CNN_34_0.png" src="../_images/2_MNIST_CNN_34_0.png" />
</div>
</div>
<p>Note that we reshaped the data in the training set into pixel format so that we can plot it to have a look. Now we need to flatten it again so that it is in the format that that model expects i.e. a vector of 784 elements. We can do this easily by calling the <code class="docutils literal notranslate"><span class="pre">.reshape()</span></code> attribute once again before finally using <code class="docutils literal notranslate"><span class="pre">model.predict()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_input</span> <span class="o">=</span> <span class="n">test_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;MLP output: &#39;</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MLP output:  [[9.9997985e-01 4.7243642e-08 1.9398398e-05 1.2118571e-13 1.9069360e-10
  1.9226991e-10 4.8101962e-09 4.8936687e-07 2.5674458e-12 2.4911907e-07]]
</pre></div>
</div>
</div>
</div>
<p>Here you can see the output of the MLP network in its raw form based on the input of the handwritten digit. Due to the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> activation layer we used in the output, the sum of the output vector equals 1. The vector is of length 10 corresponding to each of the possible classes (numbers from 0-9) that the number could be. We can therefore determine the models prediction by looking at the maximum value in this vector, in this case - the first element in the vector is the largest (by far) which corresponds to the number 0. Which is correct! This is good news, and the fact that this number was much larger is a testament to how confident the network is that this is in fact a number 0.</p>
<p>It is possible to visualise this in a better way using a bar chart, we will therefore be able to compare the predictions of the MLP network for any input MNIST digit for each possible class. Because the model was so successful, it was actually quite difficult to find a sample to input to the network to show this clearly. But the 8th sample worked nicely as it was quite ambiguos and I was unsure as to what number this was meant to be - you‚Äôll see that it looks like a number 5 but could probably also pass for a number 6. Run the code below and take a look at the output, take some time to change the <code class="docutils literal notranslate"><span class="pre">index</span></code> number and see how the bar graph changes for different MNIST digits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># reshape the image into a format the network understands</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">8</span>                            <span class="c1"># which sample (index) should we take from the test dataset to evalutate!</span>
<span class="n">test_input</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">index</span><span class="p">,:,:]</span>        <span class="c1"># [28x28] array containing image data for a single digit taken from the dataset.</span>
<span class="n">test_input</span> <span class="o">=</span> <span class="n">test_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>

<span class="c1"># create subplot to show input and its prediction</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="c1"># MNIST test input (1st subplot)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">test_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="c1"># bar chart (2nd subplot)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;2&#39;</span><span class="p">,</span> <span class="s1">&#39;3&#39;</span><span class="p">,</span> <span class="s1">&#39;4&#39;</span><span class="p">,</span> <span class="s1">&#39;5&#39;</span><span class="p">,</span> <span class="s1">&#39;6&#39;</span><span class="p">,</span> <span class="s1">&#39;7&#39;</span><span class="p">,</span> <span class="s1">&#39;8&#39;</span><span class="p">,</span> <span class="s1">&#39;9&#39;</span><span class="p">]</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="c1"># convert prediction array into list to plot on chart</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">class_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;MNIST Handwritten Digit Class&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Softmax prediction (Confidence)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Prediction of the MLP network trained on the MNIST handwritten digits dataset&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_MNIST_CNN_38_0.png" src="../_images/2_MNIST_CNN_38_0.png" />
</div>
</div>
<p>~ <code class="docutils literal notranslate"><span class="pre">Finished!</span></code></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "tensorflow"
        },
        kernelOptions: {
            kernelName: "tensorflow",
            path: "./Examples"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'tensorflow'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="1_MNIST_MLP.html" title="previous page">1 - MNIST MLP Network</a>
    <a class='right-next' id="next-link" href="../Python%20Help/Introduction%20to%20Python.html" title="next page">Introduction to Python</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Samuel Thompson<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>