

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2 - MNIST CNN Network &#8212; Ai Training Manual for Maisie &amp; Filipe</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-dropdown.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3 - LSTM Network" href="3_LSTM.html" />
    <link rel="prev" title="1 - MNIST MLP Network" href="1_MNIST_MLP.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/EdLogo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Ai Training Manual for Maisie & Filipe</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  JupyterBook
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown.html">
   Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks.html">
   Content with notebooks
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning Examples
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="1_MNIST_MLP.html">
   1 - MNIST MLP Network
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2 - MNIST CNN Network
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_LSTM.html">
   3 - LSTM Network
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Extra Stuff
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Extras/Convolutions.html">
   Convolution Filter Example
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Python Support
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Python%20Help/Introduction%20to%20Python.html">
   Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python%20Help/Useful%20Python%20Commands.html">
   Useful Packages / Commands
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Python%20Help/Glossary.html">
   Glossary
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Examples/2_MNIST_CNN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/samph4/TrainingBook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/samph4/TrainingBook/master?urlpath=tree/Examples/2_MNIST_CNN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/samph4/TrainingBook/blob/master/Examples/2_MNIST_CNN.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preface">
   Preface
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-are-cnns">
   What are CNNs?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feature-learning">
     Feature Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification">
     Classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#creating-the-cnn-algorithm">
   Creating the CNN Algorithm
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#necessary-imports">
     Necessary Imports
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading-preparing-the-training-data">
     Loading &amp; Preparing the training data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-cnn">
     Creating the CNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#results-and-evaluation">
   Results and Evaluation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#test-the-model-using-the-test-data">
     Test the model using the test data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#extra-deep-dive-into-the-filters">
   Extra: Deep dive into the filters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-filters">
     Visualising Filters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualising-feature-maps">
     Visualising Feature Maps
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="mnist-cnn-network">
<h1>2 - MNIST CNN Network<a class="headerlink" href="#mnist-cnn-network" title="Permalink to this headline">¶</a></h1>
<p>Adapted from this tutorial -&gt; https://victorzhou.com/blog/keras-cnn-tutorial/#the-full-code</p>
<p><code class="docutils literal notranslate"><span class="pre">github.com/samph4</span></code></p>
<p>~</p>
<div class="section" id="preface">
<h2>Preface<a class="headerlink" href="#preface" title="Permalink to this headline">¶</a></h2>
<p>This example will use the same MNIST dataset as we used in the last example. However, this time we are going to train a different class of neural network. This time, we’ll be using a Convolutional Neural Network (CNN). CNN’s are extremely popular recently as they have revolutionised the field of computer vision as they are well equipped to deal with and process data in image format. There is an article by Victor Zhou (https://victorzhou.com/blog/intro-to-cnns-part-1/) that does an excellent job introducing what a CNN is, but I’ll try and summarise his key points here.</p>
<p>A classic use of CNNs is to perform image classification e.g. looking at an image of a pet and determining whether it is a cat or a dog. CNN’s can achieve extraordinary results in this regard and some of the most successful and popular CNN networks achieve accuracies of over 88 % on famous image databases such as ImageNet that contains over 14 million images  belonging to ~22,000 categories (where one category might be a cat). You can quickly begin to appreciate the capabilities of these networks and the tasks they might be able to perform. It is a fantastic time to get into machine learning since large improvements in the field have been made in the last 10 years: the image below shows the progression of the ‘state of the art’ trained networks on the ImageNet database (taken from https://paperswithcode.com/sota/image-classification-on-imagenet).</p>
<p><img alt="Image" src="../_images/cnn_exp.png" /></p>
<p>Determining whether or not an image is of a cat or a dog seems straight forward, couldn’t we just use a standard neural network (MLP)? The answer is yes, but it might be an extremely inefficient solution. The reason being that images can be very big (depending on pixel size).</p>
<blockquote>
<div><p>“Images used for Computer Vision problems nowadays are often 224x224 or larger. Imagine building a neural network to process 224x224 color images: including the 3 color channels (RGB) in the image, that comes out to 224 x 224 x 3 = 150,528 input features! A typical hidden layer in such a network might have 1024 nodes, so we’d have to train 150,528 x 1024 = 150+ million weights for the first layer alone. Our network would be huge and nearly impossible to train.” - https://victorzhou.com/blog/intro-to-cnns-part-1/.</p>
</div></blockquote>
<p>This is exactly what we did in the previous example, where we took the 28x28 input image (MNIST digit) and flattened it to form a feature vector of 784 elements. This quickly gets out of hand with larger image sizes and as a result training times and convergence can suffer. Nor is it the most logical approach. Intuitively, when we consider an image we interpret the image based on a collection of pixels. Pixels are most useful in the context of their neighbours and as humans we gather much more information from a collection of pixels rather than isolated, indivudal colours. Objects in images are made up of small, <em>localised</em> features, like the circular iris of an eye or the square corner of a piece of paper. Doesn’t it seem wasteful for every node in the first hidden layer to look at every single pixel? Could it therefore be helpful to try and create something that adopts similar practicices? Also, positions can change. And this is particularly important. If we train a network that is capable of identifying a cat, we would want the model to be able to detect that cat regardless of where it appears in the image.</p>
<blockquote>
<div><p>“Imagine training a network that works well on a certain dog image, but then feeding it a slightly shifted version of the same image. The dog would not activate the same neurons, so the network would react completely differently!”. - https://victorzhou.com/blog/intro-to-cnns-part-1/.</p>
</div></blockquote>
<p>We did not run into this problem in the previous example since the MNIST dataset had been structured for us. The dataset contains small images (low # of pixels = low # of features) that, more imporantly, had been centered so we did not encounter the issue of shifting or funky orientations.</p>
</div>
<div class="section" id="what-are-cnns">
<h2>What are CNNs?<a class="headerlink" href="#what-are-cnns" title="Permalink to this headline">¶</a></h2>
<p>To get a good overview of what happens within a Convolutional Neural Network, I would strongly reccomend to watch this video first (https://www.youtube.com/watch?v=YRhxdVk_sIs). It is a relatively short video but does a good job explaining the concept and highlights some of the key points to be aware of - watching it will make the rest of this example make a lot more sense.</p>
<p><img alt="Image" src="../_images/cnn.jpeg" /></p>
<div class="section" id="feature-learning">
<h3>Feature Learning<a class="headerlink" href="#feature-learning" title="Permalink to this headline">¶</a></h3>
<p>The diagram above shows a typical schematic for a CNN. You can see that the process is split into two main categories; feature learning and classification. Let us first consider feature learning:</p>
<blockquote>
<div><p>Feature learning is a set of techniques that learn a feature: a transformation of raw data input to a representation that can be effectively exploited in machine learning tasks. - https://www.quora.com/What-is-feature-learning.</p>
</div></blockquote>
<p>In that regard, we will create the CNN architecture such that it can learn the features of the problem. You’ll notice on the diagram above that within the feature engineering categories there are different actions happening that we can take advantage when performing feature engineering. Of course, the input in this case is self explanatory as we know that we are dealing with an input of image format. But the others include convolutions, activation functions and pooling.</p>
<p><ins>Convolution</ins></p>
<p>Convolutional layers in a CNN systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input. Convolutional layers prove very effective, and stacking convolutional layers close to the input to learn low-level features (lines) and layers deeper in the model to learn high-order features or more abstract features, like shapes or specific objects.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is important to emphasise here that in the video they will have mentioned filters such as the ‘edge detection filters’ that you can convolve across an image to detect the edges. The difference is, during training the CNN network creates their OWN filters and optimises them to learn features specific to the problem and subsequently improve the loss function.</p>
</div>
<p><ins>Pooling</ins></p>
<p>A pooling layer is a new layer added after the convolutional layer. Specifically, after a nonlinearity (e.g. Relu) has been applied to the feature maps output by a convolutional layer; for example the layers in a model may look as follows:</p>
<ol class="simple">
<li><p>Input image</p></li>
<li><p>Convolutional layer</p></li>
<li><p>Activation Function (non-linearity)</p></li>
<li><p>Pooling Layer</p></li>
</ol>
<p>The pooling layer is important because it helps to make the feature representation become approximately invariant to small translations in the input. We mentioned earlier that one of the issues with a MLP network with a large input vector would be that if there were changes in position (translations/rotations) of the object you are trying to classify, then the model would struggle to recognise/classify the object as you would expect. Here, the pooling layer operates upon each feature map separately to create a new set of the same number of pooled feature maps (feature map = output from Conv layer, often there are a few maps due to 3 RGB channels etc). Pooling involves selecting a pooling operation, much like a filter, to be applied to the feature maps in a similar fashion to convolving. The size of the pooling operation or filter is smaller than the size of the feature map, typically it is almost always 2x2 pixels.</p>
<p>This means that the pooling layer will always reduce the size of each feature map by a factor of 2, e.g. each dimension is halved, reducing the number of pixels or values in each feature map to 1/4 of the size. For example, a pooling layer applied to a feature map of 6x6 (36 pixels) will result in an output pooled feature map of 3x3 (9 pixels).</p>
<blockquote>
<div><p>“In all cases, pooling helps to make the representation become approximately invariant to small translations of the input. Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.” - Page 342, Deep Learning, Ian Goodfellow 2016</p>
</div></blockquote>
<p>The pooling operation is specified and is not a learnable parameter. Two common functions used in the pooling operation are: Average pooling and Maximum pooling. Average pooling calculates the average value for each patch on the feature map, whereas maximum pooling calculates the maxmimum value for each patch on the feature map (patch refers to the 2x2 pooling filter applied to the feature map output by the Conv Layer). The result of using a pooling layer and creating down sampled or pooled feature maps is a summarized version of the features detected in the input.</p>
</div>
<div class="section" id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h3>
<p>The classification segment is almost identical to the MLP segment in the previous example. We begin by flattening the output of the final pooling layer to form a single vector; so if the final output layer is of size [16x16], we flatten this such that it becomes a vector of 256 elements. Depending on the problem, we may wish to pass this feature vector through an additional dense (fully-connected) layer which is a standard MLP layer with a user-defined number of nodes. Finally, we add a softmax layer to convert the output of the final layer in the network to a probability distribution consisting of a prediction by the model for each of the classes defined in the training dataset.</p>
<p>~</p>
</div>
</div>
<div class="section" id="creating-the-cnn-algorithm">
<h2>Creating the CNN Algorithm<a class="headerlink" href="#creating-the-cnn-algorithm" title="Permalink to this headline">¶</a></h2>
<div class="section" id="necessary-imports">
<h3>Necessary Imports<a class="headerlink" href="#necessary-imports" title="Permalink to this headline">¶</a></h3>
<p>Like we did last time, we begin by importing the packages that we will use to develop our algorithm. <code class="docutils literal notranslate"><span class="pre">Numpy</span></code> is a python library used for working with arrays and the syntax is very similar to MATLAB which makes manipulating data easy. Again, we use the <code class="docutils literal notranslate"><span class="pre">keras</span></code> library because it gives us access to machine learning functions without us having to code them all explicitly. Last time, we imported the mnist dataset through the <code class="docutils literal notranslate"><span class="pre">keras.datasets</span></code> attribute, but here we import the data directly from <code class="docutils literal notranslate"><span class="pre">mnist</span></code> by calling <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">mnist</span></code>. This can be useful because you can get access to more information about the dataset. Often we must use a particular version of keras for compatibility issues and as such might not have the most up to date information (if anything has been added since that update). Finally, we call <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> again to make our plots.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span> 
<span class="kn">import</span> <span class="nn">mnist</span> <span class="c1"># import mnist directly to access more info</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># layers used in CNN</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span>

<span class="c1"># convert scalar labels to categorical</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="loading-preparing-the-training-data">
<h3>Loading &amp; Preparing the training data<a class="headerlink" href="#loading-preparing-the-training-data" title="Permalink to this headline">¶</a></h3>
<p>Here we define the variables that contain the training and test data from the mnist dataset. If unsure about the structure of the mnist import, we can call <code class="docutils literal notranslate"><span class="pre">dir(mnist)</span></code> and we will see a whole list of attributes that we can call. Here we see that <code class="docutils literal notranslate"><span class="pre">train_images</span></code> and <code class="docutils literal notranslate"><span class="pre">test_images</span></code> has already been prepared as part of the import, so we can simply make a variable equal to <code class="docutils literal notranslate"><span class="pre">mnist.train_images()</span></code> to store the training images. It is a similar story with the labels; we can call <code class="docutils literal notranslate"><span class="pre">mnist.train_labels()</span></code> to obtain the labels for the training data. The first element here will show the class for the first image in the training set.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Always check the shape of the dataset whenever you import one! Will make it much easier to create your model architecture and avoid any incompatibilities.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_images</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train_images</span><span class="p">()</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train_labels</span><span class="p">()</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test_images</span><span class="p">()</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test_labels</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (60000, 28, 28)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (60000,)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(60000, 28, 28)
(60000,)
</pre></div>
</div>
</div>
</div>
<p>Before we begin, we’ll normalize the image pixel values from [0, 255] to [-0.5, 0.5] to make our network easier to train (using smaller, centered values usually leads to better results). We’ll also reshape each image from (28, 28) to (28, 28, 1) because Keras requires the third dimension to also be specified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Normalize the images.</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_images</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="p">(</span><span class="n">test_images</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>

<span class="c1"># # Reshape the images.</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">train_images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (60000, 28, 28, 1)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (10000, 28, 28, 1)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(60000, 28, 28, 1)
(10000, 28, 28, 1)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="creating-the-cnn">
<h3>Creating the CNN<a class="headerlink" href="#creating-the-cnn" title="Permalink to this headline">¶</a></h3>
<p>Convolutional Neural Networks do not learn a single filter. Infact, they learn multiple features in parallel for a given input. The amount of filters you would want the CNN to learn depends on the complexity of the problem. For example, if you are trying to develop a CNN classification network that can tell the difference between a dog and a whale; you would not need many filters at all as the features of dogs and whales are so vastly different and it is ‘easier’ to differentiate between the features of the animals. Differentiating between dog breeds however may be more difficult, each dog breed has 4 legs, fur, two eyes and a nose etc - but the differences between breeds may be more subtle and dependent on the proportions of the face and body etc. We would therefore require more filters in the CNN such that the model has more opportunity to create filters that ‘learn’ how to identify key features that differentiate between the different breeds (classes).</p>
<p>Here we set the number of filters to be 8, filter size as 3 and the pool size as 2. As mentioned earlier, a filter is a [NxN] array that convolves over the image/feature map. So the filter size of 3 refers to a [3x3] filter and thus this refers to 9 additional trainable parameters to optimize during training. The pool size is set at 2 which refers to a [2x2] pooling operation (filter).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_filters</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">filter_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">pool_size</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">set()</span></code> function which is a default Python function to determine each of the unique classes in this dataset. The <code class="docutils literal notranslate"> <span class="pre">set()</span></code>  function allows us to input a list/vector within its parentheses and returns a set of unique values from the input. Therefore if we write <code class="docutils literal notranslate"> <span class="pre">set(train_labels)</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">set</span><span class="p">(</span><span class="n">train_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
</pre></div>
</div>
</div>
</div>
<p>We see each of the possible classes for the MNIST dataset. Intuitively this also makes sense because 0-9 are the only numeric digits that exist. We can then call the <code class="docutils literal notranslate"> <span class="pre">len()</span></code>  function to determine the length of this set, i.e. the total number of classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">train_labels</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Total number of classes:&#39;</span><span class="p">,</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total number of classes: 10
</pre></div>
</div>
</div>
</div>
<p>Here we will actually start to define the architecture of the CNN network. Again this part shouldn’t be scary, the most import thing (as always), is making sure that we have a clear idea as to what is going into the network (input) and what is going out of it (the output). We previously formatted the training data such that the input image has a shape (28,28,1) as Keras requires the input to have three dimensions. We want the model to take an image as input and give us the classification vector as output. We know that we cant the output to be a probability distribution associated with a prediction for any given input that corresponds to one of the 10 different classes we are classifying.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">num_filters</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The structure here should be familiar and share some similarities with the MLP network that we made in the previous example. Again, we create a <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> model. This allows us to define a model architecture layer by layer, adding subsequent layers easily by using the <code class="docutils literal notranslate"><span class="pre">model.add()</span></code> attribute in Keras. We then proceed to add four layers; a <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> layer, a <code class="docutils literal notranslate"><span class="pre">MaxPooling2D</span></code> layer, a <code class="docutils literal notranslate"><span class="pre">Flatten</span></code> layer and finally a <code class="docutils literal notranslate"><span class="pre">Dense</span></code> (fully connected) layer.</p>
<p>The Conv2D class is structured as follows:</p>
<p><code class="docutils literal notranslate"><span class="pre">Conv2D(filters,</span> <span class="pre">kernel_size,</span> <span class="pre">strides=(1,</span> <span class="pre">1),</span> <span class="pre">padding='valid',</span> <span class="pre">data_format=None,</span> <span class="pre">dilation_rate=(1,</span> <span class="pre">1),</span> <span class="pre">activation=None,</span> <span class="pre">use_bias=True,</span> <span class="pre">kernel_initializer='glorot_uniform',</span> <span class="pre">bias_initializer='zeros',</span> <span class="pre">kernel_regularizer=None,</span> <span class="pre">bias_regularizer=None,</span> <span class="pre">activity_regularizer=None,</span> <span class="pre">kernel_constraint=None,</span> <span class="pre">bias_constraint=None,</span> <span class="pre">**kwargs)</span></code>.</p>
<p>Note that any parameter with an equals sign is a default property that can be modified if we require further modification of the convolution layer. The <code class="docutils literal notranslate"><span class="pre">filters</span></code> property refers to the number of filters, in our case equal to variable <code class="docutils literal notranslate"><span class="pre">num_filters</span></code> that we previously defined as 8. The <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> property refers to the dimension of the filter themselves (also known as kernels) for which we already defined variable <code class="docutils literal notranslate"><span class="pre">filter_size</span></code> as 3. Finally we must define the shape of the input as this is the first layer in the network, here the images are [28x28] pixels in size and we recall that keras required the input to be in a format with the third dimension present.</p>
<p>We add the pooling layer simply by calling <code class="docutils literal notranslate"><span class="pre">model.add(MaxPooling2D())</span></code> and we must define the size of the ‘pooling’ filter/kernel, which we previously defined as <code class="docutils literal notranslate"><span class="pre">pool_size</span></code> with a value of 2. We then flatten the feature map into a single (1D) vector of nodes (such that a 28x28 would become a vector of 784 elements). Finally, we connect this to a <code class="docutils literal notranslate"><span class="pre">Dense()</span></code> (fully connected) layer with a softmax activation function (to return probability distribution) for the defined number of classes <code class="docutils literal notranslate"><span class="pre">num_classes</span></code>.</p>
<p>The last step before training is to compile the model. We need to compile the model as this lets us define the loss function, the optimizer and the metrics used to track the results of the training process. The <code class="docutils literal notranslate"><span class="pre">model.compile()</span></code> function is powerful and allows us to change and define lots of things about the training process; here we can select whichever loss function we like, we can use any optimiser and also use any metric we wish to judge the performance of the model (see https://keras.io/api/metrics/).</p>
<p>Refer to the previous example for more information regarding this, but again we select the <code class="docutils literal notranslate"><span class="pre">adam</span></code> optimizer and the <code class="docutils literal notranslate"><span class="pre">categorical_crossentropy</span></code> function and wish to record the <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> metrics to evaluate the training process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 60000 samples, validate on 10000 samples
Epoch 1/10
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   32/60000 [..............................] - ETA: 2:32:31
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">UnknownError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">9</span><span class="o">-</span><span class="mf">302e094</span><span class="n">c995d</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)))</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training.py</span> in <span class="ni">fit</span><span class="nt">(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">817</span>         <span class="n">max_queue_size</span><span class="o">=</span><span class="n">max_queue_size</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">818</span>         <span class="n">workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span>
<span class="ne">--&gt; </span><span class="mi">819</span>         <span class="n">use_multiprocessing</span><span class="o">=</span><span class="n">use_multiprocessing</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">820</span> 
<span class="g g-Whitespace">    </span><span class="mi">821</span>   <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py</span> in <span class="ni">fit</span><span class="nt">(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">340</span>                 <span class="n">mode</span><span class="o">=</span><span class="n">ModeKeys</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">341</span>                 <span class="n">training_context</span><span class="o">=</span><span class="n">training_context</span><span class="p">,</span>
<span class="ne">--&gt; </span><span class="mi">342</span>                 <span class="n">total_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">343</span>             <span class="n">cbks</span><span class="o">.</span><span class="n">make_logs</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">epoch_logs</span><span class="p">,</span> <span class="n">training_result</span><span class="p">,</span> <span class="n">ModeKeys</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">344</span> 

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py</span> in <span class="ni">run_one_epoch</span><span class="nt">(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)</span>
<span class="g g-Whitespace">    </span><span class="mi">126</span>         <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">current_batch_size</span><span class="p">)</span> <span class="k">as</span> <span class="n">batch_logs</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">127</span>       <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">128</span>         <span class="n">batch_outs</span> <span class="o">=</span> <span class="n">execution_function</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">129</span>       <span class="k">except</span> <span class="p">(</span><span class="ne">StopIteration</span><span class="p">,</span> <span class="n">errors</span><span class="o">.</span><span class="n">OutOfRangeError</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">130</span>         <span class="c1"># TODO(kaftan): File bug about tf function and errors.OutOfRangeError?</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py</span> in <span class="ni">execution_function</span><span class="nt">(input_fn)</span>
<span class="g g-Whitespace">     </span><span class="mi">96</span>     <span class="c1"># `numpy` translates Tensors to values in Eager mode.</span>
<span class="g g-Whitespace">     </span><span class="mi">97</span>     <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">_non_none_constant_value</span><span class="p">,</span>
<span class="ne">---&gt; </span><span class="mi">98</span>                               <span class="n">distributed_function</span><span class="p">(</span><span class="n">input_fn</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">99</span> 
<span class="g g-Whitespace">    </span><span class="mi">100</span>   <span class="k">return</span> <span class="n">execution_function</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\eager\def_function.py</span> in <span class="ni">__call__</span><span class="nt">(self, *args, **kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">566</span>         <span class="n">xla_context</span><span class="o">.</span><span class="n">Exit</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">567</span>     <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">568</span>       <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">569</span> 
<span class="g g-Whitespace">    </span><span class="mi">570</span>     <span class="k">if</span> <span class="n">tracing_count</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_tracing_count</span><span class="p">():</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\eager\def_function.py</span> in <span class="ni">_call</span><span class="nt">(self, *args, **kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">630</span>         <span class="c1"># Lifting succeeded, so variables are initialized and we can run the</span>
<span class="g g-Whitespace">    </span><span class="mi">631</span>         <span class="c1"># stateless function.</span>
<span class="ne">--&gt; </span><span class="mi">632</span>         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateless_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">633</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">634</span>       <span class="n">canon_args</span><span class="p">,</span> <span class="n">canon_kwds</span> <span class="o">=</span> \

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\eager\function.py</span> in <span class="ni">__call__</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">2361</span>     <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">2362</span>       <span class="n">graph_function</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_define_function</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">2363</span>     <span class="k">return</span> <span class="n">graph_function</span><span class="o">.</span><span class="n">_filtered_call</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
<span class="g g-Whitespace">   </span><span class="mi">2364</span> 
<span class="g g-Whitespace">   </span><span class="mi">2365</span>   <span class="nd">@property</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\eager\function.py</span> in <span class="ni">_filtered_call</span><span class="nt">(self, args, kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1609</span>          <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1610</span>                            <span class="n">resource_variable_ops</span><span class="o">.</span><span class="n">BaseResourceVariable</span><span class="p">))),</span>
<span class="ne">-&gt; </span><span class="mi">1611</span>         <span class="bp">self</span><span class="o">.</span><span class="n">captured_inputs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1612</span> 
<span class="g g-Whitespace">   </span><span class="mi">1613</span>   <span class="k">def</span> <span class="nf">_call_flat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">captured_inputs</span><span class="p">,</span> <span class="n">cancellation_manager</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\eager\function.py</span> in <span class="ni">_call_flat</span><span class="nt">(self, args, captured_inputs, cancellation_manager)</span>
<span class="g g-Whitespace">   </span><span class="mi">1690</span>       <span class="c1"># No tape is watching; skip to running the function.</span>
<span class="g g-Whitespace">   </span><span class="mi">1691</span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_call_outputs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_inference_function</span><span class="o">.</span><span class="n">call</span><span class="p">(</span>
<span class="ne">-&gt; </span><span class="mi">1692</span>           <span class="n">ctx</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">cancellation_manager</span><span class="o">=</span><span class="n">cancellation_manager</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">1693</span>     <span class="n">forward_backward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_forward_and_backward_functions</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1694</span>         <span class="n">args</span><span class="p">,</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\eager\function.py</span> in <span class="ni">call</span><span class="nt">(self, ctx, args, cancellation_manager)</span>
<span class="g g-Whitespace">    </span><span class="mi">543</span>               <span class="n">inputs</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">544</span>               <span class="n">attrs</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;executor_type&quot;</span><span class="p">,</span> <span class="n">executor_type</span><span class="p">,</span> <span class="s2">&quot;config_proto&quot;</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span>
<span class="ne">--&gt; </span><span class="mi">545</span>               <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">546</span>         <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">547</span>           <span class="n">outputs</span> <span class="o">=</span> <span class="n">execute</span><span class="o">.</span><span class="n">execute_with_cancellation</span><span class="p">(</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\eager\execute.py</span> in <span class="ni">quick_execute</span><span class="nt">(op_name, num_outputs, inputs, attrs, ctx, name)</span>
<span class="g g-Whitespace">     </span><span class="mi">65</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">66</span>       <span class="n">message</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">message</span>
<span class="ne">---&gt; </span><span class="mi">67</span>     <span class="n">six</span><span class="o">.</span><span class="n">raise_from</span><span class="p">(</span><span class="n">core</span><span class="o">.</span><span class="n">_status_to_exception</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">code</span><span class="p">,</span> <span class="n">message</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">68</span>   <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">69</span>     <span class="n">keras_symbolic_tensors</span> <span class="o">=</span> <span class="p">[</span>

<span class="nn">~\miniconda3\envs\tensorflow\lib\site-packages\six.py</span> in <span class="ni">raise_from</span><span class="nt">(value, from_value)</span>

<span class="ne">UnknownError</span>:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 <span class="p">[[</span><span class="n">node</span> <span class="n">sequential</span><span class="o">/</span><span class="n">conv2d</span><span class="o">/</span><span class="n">Conv2D</span> <span class="p">(</span><span class="n">defined</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">9</span><span class="o">-</span><span class="mf">302e094</span><span class="n">c995d</span><span class="o">&gt;</span><span class="p">:</span><span class="mi">2</span><span class="p">)</span> <span class="p">]]</span> <span class="p">[</span><span class="n">Op</span><span class="p">:</span><span class="n">__inference_distributed_function_695</span><span class="p">]</span>

<span class="n">Function</span> <span class="n">call</span> <span class="n">stack</span><span class="p">:</span>
<span class="n">distributed_function</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="results-and-evaluation">
<h2>Results and Evaluation<a class="headerlink" href="#results-and-evaluation" title="Permalink to this headline">¶</a></h2>
<p>Now the model has been trained and we can evaluate the results and play around with the trained model. I mentioned in the previous example that the metrics during trained are saved within the model variable itself, and we can access the data by calling <code class="docutils literal notranslate"><span class="pre">model.history.history</span></code>. The data is stored here in a dictionary format. Calling <code class="docutils literal notranslate"><span class="pre">model.history.history</span></code> prints out the contents and we can see the metrics that have been stored, we see <code class="docutils literal notranslate"><span class="pre">loss</span></code>, <code class="docutils literal notranslate"><span class="pre">accuracy</span></code>, <code class="docutils literal notranslate"><span class="pre">val_loss</span></code> etc and access that data explicitly by using square brackets and indexing the string containing the data we want. So if we want to create a new variable containing all of the computed <code class="docutils literal notranslate"><span class="pre">loss</span></code> values during training. We can write <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">model.history.history['loss']</span></code> and use that data to perform further analysis. In this case, we are just going to plot the data to see what is happening during training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">),</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">),</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">),</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Traiing Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">),</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Accuracy&#39;</span><span class="p">)</span>

<span class="c1"># some simple formatting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch Number&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_MNIST_CNN_21_0.png" src="../_images/2_MNIST_CNN_21_0.png" />
</div>
</div>
<p>As you can see, training loss decreases rapidly (noted by the steep gradient). This is perfectly normal, as the model always learns most during the early stages of optimization. Accuracies converge after only one epoch, and still improve during the 10th, albeit slightly. Validation loss begins to convergeat around the 6th epoch. This means that training the model for longer (increasing the number of epochs) is unlikely to improve the accuracy of the model.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Often when models are unsuccessful, some clues as to what went wrong can appear when plotting this information. We can therefore go back and make changes to the model architecture or training parameters and try again.</p>
</div>
<div class="section" id="test-the-model-using-the-test-data">
<h3>Test the model using the test data<a class="headerlink" href="#test-the-model-using-the-test-data" title="Permalink to this headline">¶</a></h3>
<p>However, this was all observed from validation data. What’s best is to test it with the actual testing data that was generated earlier. Here we call the <code class="docutils literal notranslate"><span class="pre">model.evaluate</span></code> function and input the test data we created earlier, <code class="docutils literal notranslate"><span class="pre">test_images</span></code> and <code class="docutils literal notranslate"><span class="pre">test_labels</span></code> - remembering to convert the test labels into <code class="docutils literal notranslate"><span class="pre">to_categorical</span></code> format as we are using the <code class="docutils literal notranslate"><span class="pre">categorical_crossentropy</span></code> loss function. Once again we can use the verbose feature to display the progress output and print the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test the model after training</span>
<span class="n">test_results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Test results - Loss: </span><span class="si">{</span><span class="n">test_results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1"> - Accuracy: </span><span class="si">{</span><span class="n">test_results</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10000/10000 [==============================] - 1s 61us/sample - loss: 0.0829 - accuracy: 0.9738
Test results - Loss: 0.0829402305746451 - Accuracy: 97.3800003528595%
</pre></div>
</div>
</div>
</div>
<p>The results show that the accuracy of the model was 97.79 % when evaluated on the test data that it had not been trained on! This is obviously a good result especially given the simplicity of the network and the short amount of time spent training. We could modify size of the network by increasing the number of filters, modify the size of the filters, play around with pooling, use different optimisers, activation functions and loss functions in attempt to improve the accuracy of the network. We already established that training the model for longer is unlikely to yield better results as the network has began to converge.</p>
<p>Once again, let’s play around with the model that we just trained and take a look at how we can use it to make new predictions. First of all, we call the <code class="docutils literal notranslate"><span class="pre">model.predict()</span></code> function to use the model (as you might expect) to make a new prediction. The code below selects an image from the test set by defining an index and plots a bar chart that corresponds to the probability distribution produced by the softmax output. The predictions in this layer correspond to the confidence that the trained CNN believes that the input belongs to a particular class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create subplot to show input and its prediction</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>

<span class="n">index</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">test_input</span> <span class="o">=</span> <span class="n">test_images</span><span class="p">[</span><span class="n">index</span><span class="p">,:,:]</span>


<span class="c1"># MNIST test input (1st subplot)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">test_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="c1"># bar chart (2nd subplot)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;2&#39;</span><span class="p">,</span> <span class="s1">&#39;3&#39;</span><span class="p">,</span> <span class="s1">&#39;4&#39;</span><span class="p">,</span> <span class="s1">&#39;5&#39;</span><span class="p">,</span> <span class="s1">&#39;6&#39;</span><span class="p">,</span> <span class="s1">&#39;7&#39;</span><span class="p">,</span> <span class="s1">&#39;8&#39;</span><span class="p">,</span> <span class="s1">&#39;9&#39;</span><span class="p">]</span>
<span class="c1"># prepare input to cnn</span>
<span class="n">cnn_test_input</span> <span class="o">=</span> <span class="n">test_images</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
<span class="n">cnn_test_input</span> <span class="o">=</span> <span class="n">cnn_test_input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">cnn_test_input</span><span class="p">)</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="c1"># convert prediction array into list to plot on chart</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">class_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;MNIST Handwritten Digit Class&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Softmax prediction (Confidence)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Prediction of the MLP network trained on the MNIST handwritten digits dataset&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_MNIST_CNN_25_0.png" src="../_images/2_MNIST_CNN_25_0.png" />
</div>
</div>
<p>There we have it, a trained CNN that can classify handwritten digits! If you want to take it a little further, there is more information below that looks into what is happening with the filters in more detail.</p>
<p>~ Sam</p>
</div>
</div>
<div class="section" id="extra-deep-dive-into-the-filters">
<h2>Extra: Deep dive into the filters<a class="headerlink" href="#extra-deep-dive-into-the-filters" title="Permalink to this headline">¶</a></h2>
<p>When I first started developing networks, I wish that more of the tutorials that I learnt from showed you how to access the data that the model has learned so that you can see and interact with the learned parameters. So I’m going to show you a few additional things that you can do to understand your model better. First of all, we can call the <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> function to print out the architecture of the network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 26, 26, 8)         80        
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 13, 13, 8)         0         
_________________________________________________________________
flatten (Flatten)            (None, 1352)              0         
_________________________________________________________________
dense (Dense)                (None, 10)                13530     
=================================================================
Total params: 13,610
Trainable params: 13,610
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>Here you can see each of the layers clearly and the total number of trainable parameters. In the first <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> layer, we can see that the shape is [26x26] with 8 total filters that we defined earlier. The total number of parameters is 80 which is determined by the 9 trainable parameters within each [3x3] filter multiplied by the total number of filters (8) equalling 72 (8x9). There is also an additional trainable parameter associated with each filter known as the bias, therefore total parameters = 72 + 8 = 80. During training, these values are optimised to reduce the loss function and subsequently imporve the accuracy of the network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">layers</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;tensorflow.python.keras.layers.convolutional.Conv2D at 0x20ab999e448&gt;,
 &lt;tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x20ab999e688&gt;,
 &lt;tensorflow.python.keras.layers.core.Flatten at 0x20abc9e9ec8&gt;,
 &lt;tensorflow.python.keras.layers.core.Dense at 0x20ab98b83c8&gt;]
</pre></div>
</div>
</div>
</div>
<p>We can call the <code class="docutils literal notranslate"><span class="pre">model.layers</span></code> function to access the data stored within each layer within the model. This prints out four layers corresponding to the <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> layer (index 0) and the <code class="docutils literal notranslate"><span class="pre">MaxPooling2D</span></code> layer (index 1) etc. From here, we can call <code class="docutils literal notranslate"><span class="pre">dir(models.layers[0])</span></code> to see all of the attributes that we can call from the <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> layer. The <code class="docutils literal notranslate"><span class="pre">get_weights()</span></code> attribute allows us to access all of the optimised values i.e. the filters and the biases. We can call <code class="docutils literal notranslate"><span class="pre">filters,</span> <span class="pre">biases</span> <span class="pre">=</span> <span class="pre">model.layers[0].get_weights()</span></code> to create two new variables <code class="docutils literal notranslate"><span class="pre">filters</span></code> and <code class="docutils literal notranslate"><span class="pre">weights</span></code> which contain the relevant data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># retrieve weights from the second hidden layer</span>
<span class="n">filters</span><span class="p">,</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>As always, we should double check the structure of the new <code class="docutils literal notranslate"><span class="pre">filters</span></code> variable by calling <code class="docutils literal notranslate"><span class="pre">filters.shape</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">filters</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(3, 3, 1, 8)
</pre></div>
</div>
</div>
</div>
<p>We can see the shape of the <code class="docutils literal notranslate"><span class="pre">fitlers</span></code> variable is (3,3,1,8). This refers to the size of the filter (which is [3x3x1]) and the total number of filters (8), hence (3,3,1,8). We can print the filter values for the first array (index 0) by calling <code class="docutils literal notranslate"><span class="pre">filters[:,:,:,0]</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">filters</span><span class="p">[:,:,:,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[[ 0.06931005],
        [ 0.36179632],
        [ 0.28139392]],

       [[-0.83887345],
        [ 0.41776526],
        [ 0.30940893]],

       [[-0.47847265],
        [ 0.12772204],
        [ 0.46879828]]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember that the : sign is used to index all elements within that particular dimension of a variable. So here we wish to access all of the filter information for the first layer of eight (index 0).</p>
</div>
<div class="section" id="visualising-filters">
<h3>Visualising Filters<a class="headerlink" href="#visualising-filters" title="Permalink to this headline">¶</a></h3>
<p>Right now each of our 8 filters exists as a [3x3] array which doesn’t mean all that much. Here we use different methods to visualise the filters a better/more intuitive way. First we can create a grayscale colour map for each filter that is scaled such that the position of the largest value in the [3x3] array is black and the lowest value is white. The code below plots a subplot performing this operation for each of the eight filters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create subplot to show input and its prediction</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="c1"># plot first few filters</span>
<span class="n">n_filters</span><span class="p">,</span> <span class="n">ix</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_filters</span><span class="p">):</span>
    <span class="c1"># get the filter</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">filters</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">]</span>
    <span class="c1"># plot each channel separately</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># specify subplot and turn of axis</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">ix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
        <span class="c1"># plot filter channel in grayscale</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">f</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">ix</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Filter: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># show the figure</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_MNIST_CNN_38_0.png" src="../_images/2_MNIST_CNN_38_0.png" />
</div>
</div>
<p>The dark squares indicate small or inhibitory weights and the light squares represent large or excitatory weights. Using this intuition, we can see that Filter #5 on the first row has strong horizontal presence - this suggests that the filter might be suitable to detecting horizontal edges.</p>
</div>
<div class="section" id="visualising-feature-maps">
<h3>Visualising Feature Maps<a class="headerlink" href="#visualising-feature-maps" title="Permalink to this headline">¶</a></h3>
<p>Lets apply each of these filters directly to some of the MNIST digits and see what effect they have and visualise what the feature maps (output from CNN layer) within the network might look like. We import the <code class="docutils literal notranslate"><span class="pre">cv2</span></code> library here to allow us to perform 2D convolutions by calling the attribute <code class="docutils literal notranslate"><span class="pre">cv2.filter2D()</span></code> - inside the parentheses we can define the image we wish to convolve over and the filter that we wish to use. Aside from that, we are once again utilising <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> to allow us to make a subplot with multiple graphs. In this example we take the first 8 images from the test dataset (to create the rows) and apply each of the 8 filters that we trained from the 2D network to them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span> 

<span class="n">nf</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># number of filters</span>
<span class="n">nr</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># number of test images</span>

<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nr</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nf</span><span class="p">):</span>
        
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">nf</span><span class="p">,</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">test_images</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">filter2D</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">filters</span><span class="p">[:,:,:,</span><span class="n">i</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="n">j</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Filter: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">continue</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_MNIST_CNN_40_0.png" src="../_images/2_MNIST_CNN_40_0.png" />
<img alt="../_images/2_MNIST_CNN_40_1.png" src="../_images/2_MNIST_CNN_40_1.png" />
<img alt="../_images/2_MNIST_CNN_40_2.png" src="../_images/2_MNIST_CNN_40_2.png" />
<img alt="../_images/2_MNIST_CNN_40_3.png" src="../_images/2_MNIST_CNN_40_3.png" />
<img alt="../_images/2_MNIST_CNN_40_4.png" src="../_images/2_MNIST_CNN_40_4.png" />
<img alt="../_images/2_MNIST_CNN_40_5.png" src="../_images/2_MNIST_CNN_40_5.png" />
<img alt="../_images/2_MNIST_CNN_40_6.png" src="../_images/2_MNIST_CNN_40_6.png" />
<img alt="../_images/2_MNIST_CNN_40_7.png" src="../_images/2_MNIST_CNN_40_7.png" />
</div>
</div>
<p>This is a useful task to do as it gives us a much clearer indication as to what the filters are actually doing. We mentioned before that Filter #5 could be a horizontal edge detection filter, and we can see that this is actually the case as the horizontal lines that exist in the MNIST digit image are emphasised and dark.</p>
<p>Another useful task might be to take a single input image and sequentially apply each of the 8 filters to give a true representation as to what is happening inside the CNN network. In this example, we just applied each filter to the original MNIST image. It should be emphasised at this point that the initial filters of a CNN network tend to detect larger/more distinct features from the input image, and as you progress deeper into the CNN network the filters tend to detect more subtle differences. This is more obvious for more sophisticated CNNs that have more sophisticated filters that are able to identify a more complex variety of features.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "tensorflow"
        },
        kernelOptions: {
            kernelName: "tensorflow",
            path: "./Examples"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'tensorflow'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="1_MNIST_MLP.html" title="previous page">1 - MNIST MLP Network</a>
    <a class='right-next' id="next-link" href="3_LSTM.html" title="next page">3 - LSTM Network</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Samuel Thompson<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../_static/js/index.js"></script>
    
  </body>
</html>