{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Generative Adversarial Network\n",
    "\n",
    "`github.com/samph4`\n",
    "\n",
    "~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "\n",
    "This example will be more in-depth than the first few, but a lot of the principles that we have already applied also apply here. As always, we'll go through it step by step and I'll do my best to explain each part so that it makes sense and is as easy to follow as I can make it. In this final example, we will be looking at Generative Adversarial Networks - affectionately known as GANs. The concept of GANs were first introduced by Ian Goodfellow and his team in 2014 (https://arxiv.org/abs/1406.2661), where they \"proposed a new framework for estimating generative models via an an adversarial process\". I'll get into this in much more detail, but essentially what is happening here is that we are going to train two neural networks (that will be adversaries), that will compete against one another in order to improve. One will be reffered to as the Discriminator and the other will be known as the Generator. We combine both of these networks to form a combined model known as the GAN for training. Once training has been completed, we want to be able to use the *trained* Generator network independently to generate new things!\n",
    "\n",
    "![Image](./Figures/gan2.png)\n",
    "\n",
    "The image above looks rather unassuming, it is simply a row of portraits of four different people. The interesting thing however, is that none of these people actually exist. They are not real. Each of these images has been generated by a Generative Adversarial Network known as StyleGAN. StyleGAN is a sophisticated GAN that has been curated and trained by NVIDIA and represents the state-of-the-art results in data-driven unconditional generative image modelling and is an impressive testament as to the possibilities of Generative Networks. Here is another video that demonstrates the capabilities of these methods (which is only 2 minutes long so I recommend you watch it because it's v cool) - https://www.youtube.com/watch?v=p5U4NgVGAwg. With that being said, lets take a closer look as to how these things actually work.\n",
    "\n",
    "## Generative Adversarial Networks\n",
    "\n",
    "![Image](./Figures/gan1.png)\n",
    "\n",
    "The Generative Adversarial Network is a framework for estimating generative models via an adversarial process in which two neural networks compete against each other during training. It is a useful machine learning technique that learns to generate fake samples indistinguishable from real ones via a competitive game. Whilst this may sound a little confusing, the GAN is nothing more than a combined model where two neural networks are joined together; these are known as the Discriminator $D$ and the Generator $G$. The Discriminator $D$ is a classification network that is set up to maximise the probability of assigning the correct label to real (label 1) or fake (label 0) samples. Meanwhile, the Generator $G$ is trying to fool $D$ and generate new samples that the Discriminator believes came from the training set. Mathematically speaking, this corresponds to the following two-player minimax game with value function $V(G,D)$:\n",
    "\n",
    "![Image](./Figures/minmax.png)\n",
    "\n",
    "Where $x$ is the input to $D$ from the training set, $z$ is a vector of latent values input to $G$, $E_x$ is the expected value overall real data instances, $D(x)$ is the Discriminator’s estimate of the probability that real data instance $x$ is real, $E_z$ is the expected value over all random inputs to the generator and $D(G(z))$ is the discriminator’s estimate of the probability that a fake instance is real. The diagram above should help this bit make sense. To reiterate, the primary goal of $G$ is to fool $D$ and generate new samples that $D$ believes came from the training set (real). The primary goal of $D$ is to correctly classify real/fake samples by assigning a label of 0 to generated samples indicating a fake, and a label of 1 to true samples indicating that it is real and came from the training set. The training procedure for $G$ is to maximise the probablity of $D$ making a mistake i.e. an incorrect classification. In the space of arbitrary functions $G$ and $D$, a unique solution exists, with $G$ able to reproduce data with the same distribution as the training set and the output from $D$ ≈ 0.5 for all samples, which simply indicates that the discriminator can no longer differentiate between the training data and the data generated by $G$. Or in other words, the Generator $G$ has got so good at generating 'fake' data that $D$ can no longer tell the difference. The image below is taken from Google's training documentation about GANs and is worth a read as it (obviously) does a good job at explaining some of these concepts (https://developers.google.com/machine-learning/gan).\n",
    "\n",
    "![Image](./Figures/forge.png)\n",
    "\n",
    "~\n",
    "\n",
    "\n",
    "## Training Set\n",
    "\n",
    "First of all, we need to decide what we want our generative network to generate. Of course, NVIDIA's sophisticated StyleGAN is capable of generating human faces, but GANs are capable of generating new data regardless of the form that it comes in. GANs can be used to generate new audio signals, new images, new time-series data etc. GANs are capable of generating new data that is representative of the data that it was trained on (the training set). Therefore, in large, a key factor in the success of the GAN model lies in the quality of the training set. In this example, we will create a simple training set from the function $y=sin(x)$ and use the trained generator to produce similar values!\n",
    "\n",
    "```{note}\n",
    "Throughout this example I may use terms such as 'real' and 'fake' when referring to data. Real refers to data samples that come from the training set and 'fake' samples refer to any data that is produced by the Generator.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "As always, we'll start by importing the necessary libraries that we will use for this example. All of these functions we have used in previous examples so nothing should be too out of the ordinary, the `optimizers` and `initializers` however we will discuss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                                      # for working with arrays/data\n",
    "from matplotlib import pyplot                           # for plotting!\n",
    "from numpy.random import rand                           # explicit random number function\n",
    "from numpy.random import randn                          # another explicit random function\n",
    "from keras.models import Sequential                     # sequential model to create layered networks\n",
    "from keras.layers import Dense                          # Dense layer (fully-connected MLP)\n",
    "from keras.layers import LeakyReLU                      # LReLU Activation Function\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training (Target) Dataset\n",
    "\n",
    "In the previous examples we had used the MNIST handwritten digits dataset. This is nice because not only is it a comprehensive dataset, but it is also labelled, properly structured, the images have been transformed/resized to give the machine learning algorithm the best chance to learn. But of course, depending on the problem that you are considering there will not always be a perfect dataset available. More often than not, you will have to create a training set yourself and structure it appropriately. Again, in this example we are just going to use the function $y=sin(x)$ to create a training set given a range of $x$ values between 0 and 2$\\pi$. \n",
    "\n",
    "The training data will exist as a 2-D array consisting of the $x$ and corresponding $y$ values of the $sin(x)$ function. We can start by creating a new variable called `dataset` and preallocate an array of zero values with the number of rows dependant on the number of samples we would like, in this case I choose 20, and 2 columns for the $x$ and corresponding $y$ values. We use the `np.linspace()` function to create an array with `n1` equally spaced samples between 0 and 2$\\pi$ and allocate them to the first (index = 0) column of the preallocated `dataset` array. We then use the `np.sin()` function to compute the sin values of each of the values in the first column and allocate them to the second column (index=1) of the `dataset` array. Finally, the `pyplot.scatter()` function from the `matplotlib` library is used to plot the training set. Of course, if you would like to change the number of values in this dataset you can do so by changing the value of `n1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "C:\\Users\\Samuel\\Documents\\samph4\\TrainingBook\\_build\\jupyter_execute\\Examples\\4_GAN_5_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n1 = 50\n",
    "dataset = np.zeros((n1,2))\n",
    "dataset[:,0] = np.linspace(0,2*np.pi,n1)\n",
    "dataset[:,1] = np.sin(dataset[:,0])\n",
    "\n",
    "pyplot.scatter(dataset[:,0],dataset[:,1], marker='x',color='r')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions in Python\n",
    "\n",
    "The next section will list and talk through a bunch of different functions that we use in the training algorithm, but lets discuss how they work first. The majority of these functions are used to prepare the inputs to the Generator $G$ and the Discriminator $D$. If you have worked with functions in MATLAB then you might recall the syntax looks something like `function [y1,...,yN] = function_name(x1,...,xN)` where $x$ refers to the input variables and $y$ refers to the output variables. These functions are useful because there are often blocks of code that we might wish to repeat, and isntead of repeating the entire block of code, it is much better practice to remove the clutter and call a function that computes that block of code. Also, any of the variables created within the function that are not required as output are not saved as a global variable and will not take up unecessary memory.\n",
    "\n",
    "In Python, the functionality is the same but the syntax is a little different. In order to define a function, we call `def` before naming the function. Take a look at the simple example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_function(a,b):\n",
    "    c = a + b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we name the function `simple_function()`. You will notice that the function takes two inputs `a` and `b` and we define that by including them in the function parentheses and separating them with commas.\n",
    "\n",
    "```{note}\n",
    "Note the colon after the function, this is important as it initalises the function and lets Python know that the next bit of code you write will define what the function will actually do!\n",
    "```\n",
    "\n",
    "The outputs of the function are defined by the `return` statement. In this function we perform a simple addition of two input variables `a` and `b` and label that variable `c`. Adding `return c` at the end of the function does two things, it tells Python that the function statement is complete and also that we wish to return the variable `c` for use in the rest of our code! Note that the function here could be much more complicated with many more operations and variables within - if we wish to return additional variables for use in the rest of the code, we can simply return more values such as `return c, d, e, f, g` etc. Now we have defined a function, lets use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = 1; b = 2 \n",
    "c = simple_function(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we can define variables `a` and `b` and insert them into the `simple_function()` that we defined previously. We can set this function equal to a new variable `c` to store the output from that function in variable `c`. If we print `c`, as expected the value is 3 (1+2). This is fine and it works, but there are a few more things you should be aware about. Python functions are very flexible! When we defined the function earlier, we named the variables `a` and `b`, but you'll see that it doesn't actually matter and we could have named them anything. How the variables are defined are only important in the function definition itself, so you'll see that this also works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_function(4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, Python functions allow you to set default values. You may have already noticed that when you import functions from libraries and call the `help()` function on them, you'll see a whole bunch of additional input options that you may or may not need/use. These parameters are often default that don't require changing for the function to work, but if you did wish to modify the function further then the option is there to change them. Lets define another function to explain that a little better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_function(a,b,c=4):\n",
    "    d = a + b + c \n",
    "    return d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a new function that is very similar to the first, although this time we set a default parameter `c` as input and set a value of 4. We then define a new variable `d` within the function that is the sum of inputs `a`, `b` and `c` and return it for use in the rest of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_function(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we call `new_function(1,2)` it returns a value of 7. This is because it is adding the values that we insert into the function (1,2) with the default parameter `c` which was equal to 4. This returns a value of 7 which is correct! We may wish to modify a default variable in the function (something we will do regurlarly when using the Keras machine learning functions) and we can easily do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_function(1,2,c=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that we can simply override the default parameter by redefining its value when we call the function. By calling `new_function(1,2,c=10)` we replace the default `c` value of 4 to a value of 10. As expected, the function then returns a new value of 13 (1+2+10). To sum up, functions are a tool that are there to allow us to repeat complicated bits of code simply by calling a new function. There are two different types of input a function may require, ones that must be defined (such as `a` and `b` in this case) and ones that are default (`c`). It is important to understand that you do not have to define default values, if you don't the function will operate with the default values that have been preset. But if you try use a function without defining one of the essential values then the function will return an error message! But they're not that bad and once you get used to using and interacting with them you'll struggle to image a world where you didn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used in the GAN training algorithm\n",
    "\n",
    "Here we will define a bunch of the functions used for the GAN training algorithm. Although some of them might be more complicated than the simple example we considered, the idea behind them is exactly the same.\n",
    "\n",
    "#### Take Real Samples\n",
    "To evaluate the performance of the GAN we will use the real data from the training set to train the Discriminator so that it can learn the characteristics of data that comes from the training set. This will make it easier for the Discriminator to label samples that come from the Generator as fake. We therefore create a simple function named `take_real_samples()` as it is simply taking real samples from the training set and will allow us to input them into the Discriminator for training. You will notice that the function returns two values; `X` and `y`. `X` refers to the values extracted from the training set and `y` is an array of ones to label the data as real!\n",
    "\n",
    "Note, the first part of this function extracts a pre-defined number (n) of values randomly from the dataset. This is not necessary and you can definitely pass through the entire dataset on each iteration if you wish - this simply just shows a way of mixing things up and can sometimes be a useful technique when training networks. In NVIDIA's StyleGAN they intentionally add noise to different layers of their neural networks during training and find that the output from the trained networks become more flexible and less specific. This is often referred to as Mode Collapse and the extract from Google (below) does a good job at explaining what it means in reference to a GAN that generates faces:\n",
    "\n",
    "> \"Usually you want your GAN to produce a wide variety of outputs. You want, for example, a different face for every random input to your face generator. However, if a generator produces an especially plausible output, the generator may learn to produce only that output. In fact, the generator is always trying to find the one output that seems most plausible to the discriminator. If the generator starts producing the same output (or a small set of outputs) over and over again, the discriminator's best strategy is to learn to always reject that output. But if the next generation of discriminator gets stuck in a local minimum and doesn't find the best strategy, then it's too easy for the next generator iteration to find the most plausible output for the current discriminator. Each iteration of generator over-optimizes for a particular discriminator, and the discriminator never manages to learn its way out of the trap. As a result the generators rotate through a small set of output types. This form of GAN failure is called mode collapse.\" - Google GAN Tutorial (https://developers.google.com/machine-learning/gan/problems) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_real_samples(n):\n",
    "    #np.random.seed(30)\n",
    "    idx = np.random.randint(len(dataset), size=int(n))\n",
    "    X = dataset[idx,:]\n",
    "    y = np.ones((n,1)) \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate points in latent space as input for the generator\n",
    "\n",
    "Next, we define another function that sorts out the input to the Generator $G$. The input to the Generator in the GAN is known as the latent input. This is a weird name, but it is nothing more than a random input vector that we insert into the Generator. This is useful as it assures that the output from $G$ is varied and is the reason for the stochastic nature of the generative network.\n",
    "\n",
    "> Stochastic - \"Having a random probability distribution or pattern that may be analysed statistically but may not be predicted precisely.\"\n",
    "\n",
    "We define the `create_latent_points()` function below to generate this 'latent' input that we will pass into the Generator to generate new samples. It takes inputs `latent_input` and `n` that we will define later. We imported the `randn` function earlier directly from the `numpy.random` module, equally we could call `np.random.randn` for the same result. `randn` returns a sample (or samples) from the \"standard normal\" distribution. The function returns `x_input` which will feed into $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def create_latent_points(latent_dim, n):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Generator to generate n fake examples, with class labels\n",
    "\n",
    "The `generate_fake_samples()` function below calls the `create_latent_points()` function within that function, it is an example of a nested function which again you may or may not be familiar with from MATLAB. It collects the `x_input` from the `create_latent_points` function we defined earlier and inserts it directly into the `generator` model. It is important to remember that this example will take us through the creation of a GAN model and that GAN model is nothing more than a combination of a Discriminator and a Generator (two neural networks). As part of the training algorithm we will work directly with the $G$ and $D$ models and in this function we call the `predict` attribute from the `generator` variable that contains all of the information about the generator model. The `predict` attribute, as the name suggests, is the attribute that lets us actually use the generator to make predictions (generate new data). Here we use it to generate `n` fake samples and store them in variable `X`. As with the `take_real_samples`, we create another vector that stores the class label of 1 indicating fake samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "    # generate points in latent space\n",
    "    x_input = create_latent_points(latent_dim, n)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = np.zeros((n, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GAN Model\n",
    "\n",
    "Here we will define the structure of the neural networks. Remember that a GAN consists of two neural networks, a Discriminator $D$ and a Generator $G$. In this example, we are dealing with numeric data so a standard Multi-Layer Perceptron (MLP) network makes sense. We will define each network individually before creating a combined GAN model. As always, it is important for us to consider the shape of the input/output entering and leaving the networks. In the case of $D$, each input sample will be a vector containing two values - one for the $x$ value and another for its corresponding $y$ value - therefore 2 nodes in the input layer of the MLP network. Recall that $D$ is nothing more than a classification network, it's output will therefore be a single node that outputs a value between 0 and 1 corresponding to the confidence that it has that a sample came from a real (label = 1) or a fake (label = 0) distribution. The $G$ network is the latent input we discussed earlier. Recall that the latent input is nothing more than a vector of random numbers, so it's size will be equal to variable `latent_dim` which we can define later. The primary goal of $G$ is to generate samples that $$ believes came from the training set, and a single sample consists of an $x$ value and a corresponding $y$ value. Therefore the output of $G$ simply has two nodes in its output layer.\n",
    "\n",
    "\n",
    "\n",
    "#### Discriminator\n",
    "\n",
    "Again we use Keras' `Sequential()` model as it makes it very simple to add and play around with the layers in the network. The layers will look familiar to ones we have used in the previous examples, again the `Dense()` layer refers to a fully connected layer in which all the nodes in that layer are 'fully connected' to the ones in the previous and subsequent layer. \n",
    "\n",
    "```{note}\n",
    "Note that here we define the discriminator in a function, of course the model could be made separately outside of the function but this allows us to exectute the whole algorithm later and if we change some parameters we will not have to execute this block of code again to reinitalise the model.\n",
    "```\n",
    "\n",
    "The function takes a default input that, unless changed, is of size 2 (as discussed earlier). The output layer includes a `Sigmoid` activation function that ensures that the output is modulated between 0 and 1. The key difference between this example and others is that here we have included a `kernel_initializer`. Initializers allow us to define the way to set the initial random weights of keras layers (see keras docs for more info - https://keras.io/api/layers/initializers/ - see this link for more info regarding `he_uniform` - https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeUniform). As always, we must remember to `compile` the model to define the loss function we will use, the optimizer and whatever metrics we may wish to track during training. As we defined the discriminator within a function, we have to remember to `return model` such that the actual model parameters can be stored and used elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(n_inputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25,  kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(15,  kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(10,  kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(5,  kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator\n",
    "\n",
    "Similarly, we define the Generator $G$. Again we define it as a function taking inputs `latent_dim` and a default number of outputs, `n_outputs` equal to 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined GAN Model\n",
    "\n",
    "Now that we have defined the $G$ and $D$ networks, we can combine them to form the combined GAN model. Again, we will use a function to do this and as you might expect the only inputs that it takes are the `generator` and the `discriminator`. We use Keras' `Sequential()` model and call the `model.add` attribute to add the entire model (in the same we add individual layers we can add entire networks!). Again we compile the model to define the loss function, optimizer and metrics we wish to track during training. You may have noticed that we include `disciminator.trainable = False`, the reason for this wil be explained in more detail shortly, but in short it is because the combined GAN model is used to train the Generator only. In order to train the Geneator and update it's parameters, the loss function requires $D(G(z))$ which corresponds to the Discriminators classification of the output of the Generators output given latent input $z$. The primary goal of $G$ is to generate fake samples that the Discriminator believes to be real, therefore for a given latent input $z$, we want the Discriminator to believe that a sample $G(z)$ has the same represenation as an input sample from training set $x$. Therefore a combined model is necessary such that we can optimise $G$ based on the output of $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance\n",
    "\n",
    "At this point, the bulk of the code is written. We have defined functions that let us extract data from the training set and label it as real data accordingly (label = 0), a function that defines a random latent input from a Gaussian distribution, and finally a function that allows us to use $G$ to make predictions! We have also defined the acrhitecture of the Discriminator $D$, Generator $G$ and the combined GAN model. We can now start to develop the algorithm that controls the training procedure by defining a `train()` function and we will monitor the performance of the network during training with an additional function named `check_performance()`.\n",
    "\n",
    "First, lets consider the `check_performance()` function. You can see that it takes inputs `epoch`, `generator`, `discriminator`, `latent_dim` and a default setting `n=10`. Variables `epochs` and `latent_dim` will be defined later. The function begins by calling the `take_real_samples(n)` function to extract real samples from the training set. We make the function equal to `x_real` and `y_real` as the function returns two variables, the first of which is the training data itself, and the second is the corresponding label (label=1 for real data). The value `n` simply dictates the number of samples we take from the training set. If `n` > than the number of samples in the training set, it will simply take all of the values from the training set. Next we call the `discriminator.evaluate()` attribute from the $D$ model to evaluate the performance of $D$ at that point during the training process. The `discriminator.evaluate()` function is useful as the model will not train on any data passed into the network. It takes both the training data and the label as input, we set the `verbose=0` simply to prevent any output. The evaluate function returns the values of any metrics that are being tracked, if we call `discriminator.metrics_names` we will see `['loss', 'accuracy']` printed. We can choose to monitor either, neither or both of these values if we wish. But lets say that we just want to monitor the accuracy, we can use the `_` command to ignore the first returned variable (in this case the loss) so that `_, acc_real` will only return the accuracy from that evaluation of the discriminator. We do a similar thing by calling `generate_fake_samples()` to store the fake data generated by $G$ and the fake label (label=0). We then call the `discriminator.evaluate()` once again but this time we evaluate the discriminator on the fake samples and test its performance! Again, we use the `_` command in Python to ignore the first returned variable (loss) and we stored the accuracy of the discriminator on the fake data (from generator) into variable `acc_fake`. We can then call the `print()` function to output the accuracies during training for that given epoch number. Finally, we can use the `matplotlib` library to plot some of the results. We plot the `real` data we used from the training set (as red crosses) and the `fake` data too! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the discriminator and plot real and fake points\n",
    "def check_performance(epoch, generator, discriminator, latent_dim, n=10):\n",
    "    # prepare real samples\n",
    "    x_real, y_real = take_real_samples(n)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n*4)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print(epoch+1, acc_real, acc_fake)\n",
    "    # scatter plot real and fake data points\n",
    "    pyplot.scatter(x_real[:, 0], x_real[:, 1], marker='x', color='red')\n",
    "    pyplot.scatter(x_fake[:, 0], x_fake[:, 1], marker='$\\u25EF$', color='pink')\n",
    "    pyplot.xlim([0, 6])\n",
    "    pyplot.ylim([-1.5, 1.5])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function we need to define is the `train` function itself. This function is an integral part of the algorithm and pieces together all of the functions that we have previously defined. As a result, it takes quite a few inputs in order to make it work. As you would expect, it requires information about the models so the `generator`, `discriminator` and the combined `gan_model` are called. Again, we need to define the size of the latent input to $G$ so `latent_dim` is also defined. The default variables include `n_epochs = 50000`, `n_batch=256` and `check_it=1000`. `n_epochs` defines the maximum number of epochs that the model will train for, in this case it is set to 50,000 but this is arbitrary. Obviously you can train for shorter or longer if you wish. `n_batch` determines the number of samples that are input to the network in that iteration, this is important when working with big data as you may need to lower the batch size so that you do not exceed the memory limits of your hardware (CPU/GPU). `check_it` is another default value that we use to trigger the `check_performance` function. \n",
    "\n",
    "We start the function by creating a new variable `half_batch`. This `half_batch` variable is used to split the batch_size in half and is exclusively used to train $D$. This is because we train $D$ on both the real data (from training set) and the fake data (from the generator), if we didn't half the batch size, then the discriminator would be exposed to twice as as much data as the generator and would therefore have a competitive advantage whereby the discriminator learns faster than generator and outcompetes the network. This makes it even more difficult for $G$ to produce samples that $D$ believes to be real and as such can result in unsuccessful training.  With this being said, to intiate the training algorithm we loop over the `n_epochs` that we defined. Only once 50,000 epochs have been met can we exit the loop and cease training.\n",
    "\n",
    "```{tip}\n",
    "We could include `break` statements into the loop a way to add additional stopping criteria such that once certain user-defined conditions have been met, the training will be terminated.\n",
    "```\n",
    "\n",
    "Inside the loop, we prepare our real and fake data in the same we defined in the previous function. However, this time we will actually use this data to train and update the parameters of the network. We train $D$ by calling the `discriminator.train_on_batch()` attribute from Keras. This function requires both the training data and its associated label. Therefore we call this function twice, first on the real data and next on the fake data! Next, we want to train the Generator $G$. We define the `x_gan` variable from the function `create_latent_points()` that we defined earlier that contains the latent input that we will input to the $G$. Similarly, we define `y_gan` as the label data for the fake samples!\n",
    "\n",
    "```{note}\n",
    "Remember that the naming convention typically used refers to X as the input, and Y as the output.\n",
    "```\n",
    "\n",
    "We can then call the same `train_on_batch` attribute on the $G$ model to train the generator. Note that within the GAN model we made the layers of the discriminator untrainable so they will not update during this step! Finally, we define a condition where if the current interation is a multiple of `check_it` (equal to 1,000), then it will execute the `check_performance()` which prints all the graphs etc and some of the performance metrics during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(generator, discriminator, gan_model, latent_dim, n_epochs=50000, n_batch=100, check_it=1000):\n",
    "    # determine half the size of one batch, for updating the discriminator\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare real samples\n",
    "        x_real, y_real = take_real_samples(half_batch)\n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
    "        # update discriminator\n",
    "        discriminator.train_on_batch(x_real, y_real)\n",
    "        discriminator.train_on_batch(x_fake, y_fake)\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = create_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan)\n",
    "        # evaluate the model every n_eval epochs\n",
    "        if (i+1) % check_it == 0:\n",
    "            check_performance(i, generator, discriminator, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Model!\n",
    "\n",
    "Running this cell block will initiate and execute the algorithm. We start by (finally) defining `latent_dim` which is simply the size of the random Gaussian vector that we input to $G$. Next we decide the names that we will use for each of the models, (creatively named `discriminator`, `generator` and `gan_model`). Finally calling the `train` function is the last step to execute the algorithm! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": " Blas GEMM launch failed : a.shape=(32, 10), b.shape=(10, 15), m=32, n=15, k=10\n\t [[node dense_6/MatMul (defined at C:\\Users\\Samuel\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_377]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-49a8c05ed17a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mgan_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-ad3cfc8f0839>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, gan_model, latent_dim, n_epochs, n_batch, check_it)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mx_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtake_real_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# prepare fake examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mx_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhalf_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m# update discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-09e284b18cef>\u001b[0m in \u001b[0;36mgenerate_fake_samples\u001b[1;34m(generator, latent_dim, n)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_latent_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# predict outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# create class labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Blas GEMM launch failed : a.shape=(32, 10), b.shape=(10, 15), m=32, n=15, k=10\n\t [[node dense_6/MatMul (defined at C:\\Users\\Samuel\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_377]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 10\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = generate_fake_samples(generator, 10, 10000)\n",
    "pyplot.scatter(A[:,0],A[:,1], marker='x',color='r')\n",
    "pyplot.ylim([-1.5,1.5])\n",
    "pyplot.xlim([0,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}